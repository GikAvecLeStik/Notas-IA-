
6 parametros ---> 60 datos
mil parametros --> 10,000 datos

¿Por qué?
mas compleja mi explicacion , la tienes que susentar con datos

error en mis datos sea pequeño --> Ein(h) = 0
Ein(h) = Eout(h)  
el error que tenga = al error de salida

para aprender necesitas:
1. tener un modelo
2. tener muchos datos en funcion de tu modelo
3. que el modelo genere un modelo con error cerca a 0.

  ** Arboles de decision **
los arboles de decision explican los datos
Es un algoritmo recursivo

No puedes encontrar el arbol mas chico, es un problema NP

> la receta de este algoritmo es como escoger la variable:

Entropy
"More uncertainty, more entropy!"
cuando todos los objetos pertenecen a la misma clase, todo esta perfectamente
ordenado

si la entropia es maxima, esta completamente desordenado

P()

H(Y) = -5/6 log2 5/6 - 1/6 log2 1/6
     = 0.65

Entropia condicional

Ganancia de informacion
* decrease in entropy (uncertainty) after splitting
IG(X) = H(Y) - (H|Y)

Learning decision trees

Cuando parar:
cuando todos los datos esten bien clasificados
cuando no haya datos

cuando la IG = 0, estan completamente desordenados
IG = 0 --> no es un criterio para parar










 





